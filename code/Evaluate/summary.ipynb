{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d624e7-c91e-465a-9d4a-d9077e60032a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "\n",
    "from torchinfo import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9227ff27-0823-4615-9911-092ecd65162c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class VAEEncoder(nn.Module):\n",
    "    def __init__(self, in_channels=1, latent_dim=32, num_classes=3, label_embedding_dim=16):\n",
    "        super(VAEEncoder, self).__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes, label_embedding_dim)\n",
    "        self.conv1 = nn.Conv3d(in_channels + label_embedding_dim, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv3d(16, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv3d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4_mu = nn.Conv3d(64, latent_dim, kernel_size=4, stride=1, padding=0)\n",
    "        self.conv4_log_var = nn.Conv3d(64, latent_dim, kernel_size=4, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x, label_embedding):\n",
    "        label_embedding = label_embedding.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "        label_embedding = label_embedding.expand(-1, -1, x.shape[2], x.shape[3], x.shape[4])\n",
    "        x = torch.cat([x, label_embedding], dim=1)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        mu = self.conv4_mu(x)\n",
    "        log_var = self.conv4_log_var(x)\n",
    "        return mu, log_var\n",
    "\n",
    "\n",
    "class VAEDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim=32, out_channels=1, num_classes=3, label_embedding_dim=16):\n",
    "        super(VAEDecoder, self).__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes, label_embedding_dim)\n",
    "        self.conv_trans1 = nn.ConvTranspose3d(latent_dim + label_embedding_dim, 32, kernel_size=4, stride=1, padding=0)\n",
    "        self.conv_trans2 = nn.ConvTranspose3d(32, 16, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv_trans3 = nn.ConvTranspose3d(16, 4, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_trans4 = nn.ConvTranspose3d(4, out_channels, kernel_size=4, stride=2, padding=1)\n",
    "        self.output_layer = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, z, label_embedding):\n",
    "        \n",
    "        label_embedding = label_embedding.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "        label_embedding = label_embedding.expand(-1, -1, z.shape[2], z.shape[3], z.shape[4])  \n",
    "        z = torch.cat([z, label_embedding], dim=1)\n",
    "        z = F.gelu(self.conv_trans1(z))\n",
    "        z = F.gelu(self.conv_trans2(z))\n",
    "        z = F.gelu(self.conv_trans3(z))\n",
    "        z = self.conv_trans4(z)\n",
    "        return self.output_layer(z)\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, in_channels=1, latent_dim=32, out_channels=1, num_classes=3, label_embedding_dim=16):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = VAEEncoder(in_channels, latent_dim, num_classes, label_embedding_dim)\n",
    "        self.decoder = VAEDecoder(latent_dim, out_channels, num_classes, label_embedding_dim)\n",
    "\n",
    "    def forward(self, x, label_embedding):\n",
    "        mu, log_var = self.encoder(x, label_embedding)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        recon_x = self.decoder(z, label_embedding)\n",
    "        return recon_x, mu, log_var\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "\n",
    "class VAEGenerator(nn.Module):\n",
    "    def __init__(self, in_channel=1, out_channel=1, latent_dim=32, num_classes=3, label_embedding_dim=16):\n",
    "        super(VAEGenerator, self).__init__()\n",
    "        self.vae = VAE(in_channel, latent_dim, out_channel, num_classes, label_embedding_dim)\n",
    "\n",
    "    def forward(self, x, label_embedding):\n",
    "        recon_x, mu, log_var = self.vae(x, label_embedding)\n",
    "        return recon_x, mu, log_var\n",
    "\n",
    "\n",
    "\n",
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self, in_channel=1, out_channel=1, num_classes=3, label_embedding_dim=16):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes, label_embedding_dim)\n",
    "        self.conv_net = nn.Sequential(\n",
    "            \n",
    "            nn.Conv3d(in_channels=in_channel + label_embedding_dim, out_channels=64, kernel_size=4, stride=2, padding=1),\n",
    "            torch.nn.InstanceNorm3d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv3d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            torch.nn.InstanceNorm3d(32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv3d(32, 16, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.InstanceNorm3d(16),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv3d(16, 4, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.InstanceNorm3d(4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv3d(4, out_channel, kernel_size=4, stride=1, padding=0),\n",
    "        )\n",
    "\n",
    "    def forward(self, z, label_embedding):\n",
    "        label_embedding = label_embedding.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "        label_embedding = label_embedding.expand(-1, -1, z.shape[2], z.shape[3], z.shape[4])\n",
    "        z = torch.cat([z, label_embedding], dim=1)\n",
    "        return self.conv_net(z)\n",
    "\n",
    "\n",
    "def gradient_penalty(critic, real, fake, label_embedding, device=\"cpu\"):\n",
    "    BATCH_SIZE, C, H, W, D = real.shape\n",
    "    beta = torch.rand((BATCH_SIZE, 1, 1, 1, 1), device=device)\n",
    "    beta = beta.expand_as(real)\n",
    "    interpolated_images = real * beta + fake.detach() * (1 - beta)\n",
    "    interpolated_images.requires_grad_(True)\n",
    "\n",
    "    mixed_scores = critic(interpolated_images, label_embedding)\n",
    "\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated_images,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradient = gradient.view(BATCH_SIZE, -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "\n",
    "    return torch.mean((gradient_norm - 1) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0c163f-449c-46f1-85e6-7596ff1883c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "input_shape = (2, 1, 128, 128, 128)\n",
    "\n",
    "\n",
    "vae_generator = VAEGenerator(\n",
    "    in_channel=1, \n",
    "    out_channel=1, \n",
    "    latent_dim=32, \n",
    "    num_classes=3, \n",
    "    label_embedding_dim=16\n",
    ").to(device)\n",
    "\n",
    "discriminator = Discriminator(\n",
    "    in_channel=1, \n",
    "    out_channel=1, \n",
    "    num_classes=3, \n",
    "    label_embedding_dim=16\n",
    ").to(device)\n",
    "\n",
    "\n",
    "batch_size = 2\n",
    "in_channels = 1\n",
    "D, H, W = 128, 128, 128\n",
    "x_dummy = torch.randn(batch_size, in_channels, D, H, W).to(device)\n",
    "label_dummy = torch.randint(0, 3, (batch_size,)).to(device)\n",
    "label_embedding_dummy_gen = vae_generator.vae.encoder.label_emb(label_dummy)\n",
    "label_embedding_dummy_disc = discriminator.label_emb(label_dummy)\n",
    "\n",
    "\n",
    "def calculate_model_memory(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    total_size_bytes = total_params * 4  \n",
    "    total_size_MB = total_size_bytes / (1024 ** 2)\n",
    "    return total_params, total_size_MB\n",
    "\n",
    "\n",
    "print(\"VAEGenerator:\")\n",
    "summary(\n",
    "    vae_generator,\n",
    "    input_data=(x_dummy, label_embedding_dummy_gen),\n",
    "    depth=5,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(\"\\nDiscriminator:\")\n",
    "summary(\n",
    "    discriminator,\n",
    "    input_data=(x_dummy, label_embedding_dummy_disc),\n",
    "    depth=5,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summary",
   "language": "python",
   "name": "summary"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
