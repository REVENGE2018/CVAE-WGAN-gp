{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "acea78e3-f16e-4d59-8bf3-d93eb21181a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.linalg import sqrtm\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from skimage.filters import threshold_otsu\n",
    "from torchvision.models import inception_v3\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19b730f-6ab9-4f57-9e8d-f7272efbb659",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VAEEncoder(nn.Module):\n",
    "    def __init__(self, in_channels=1, latent_dim=32, num_classes=3, label_embedding_dim=16):\n",
    "        super(VAEEncoder, self).__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes, label_embedding_dim)\n",
    "        self.conv1 = nn.Conv3d(in_channels + label_embedding_dim, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv3d(16, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv3d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4_mu = nn.Conv3d(64, latent_dim, kernel_size=4, stride=1, padding=0)\n",
    "        self.conv4_log_var = nn.Conv3d(64, latent_dim, kernel_size=4, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x, label_embedding):\n",
    "        label_embedding = label_embedding.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "        label_embedding = label_embedding.expand(-1, -1, x.shape[2], x.shape[3], x.shape[4])\n",
    "        x = torch.cat([x, label_embedding], dim=1)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        mu = self.conv4_mu(x)\n",
    "        log_var = self.conv4_log_var(x)\n",
    "        return mu, log_var\n",
    "\n",
    "\n",
    "class VAEDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim=32, out_channels=1, num_classes=3, label_embedding_dim=16):\n",
    "        super(VAEDecoder, self).__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes, label_embedding_dim)\n",
    "        self.conv_trans1 = nn.ConvTranspose3d(latent_dim + label_embedding_dim, 32, kernel_size=4, stride=1, padding=0)\n",
    "        self.conv_trans2 = nn.ConvTranspose3d(32, 16, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv_trans3 = nn.ConvTranspose3d(16, 4, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_trans4 = nn.ConvTranspose3d(4, out_channels, kernel_size=4, stride=2, padding=1)\n",
    "        self.output_layer = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, z, label_embedding):\n",
    "        \n",
    "        label_embedding = label_embedding.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "        label_embedding = label_embedding.expand(-1, -1, z.shape[2], z.shape[3], z.shape[4])  \n",
    "        z = torch.cat([z, label_embedding], dim=1)\n",
    "        z = F.gelu(self.conv_trans1(z))\n",
    "        z = F.gelu(self.conv_trans2(z))\n",
    "        z = F.gelu(self.conv_trans3(z))\n",
    "        z = self.conv_trans4(z)\n",
    "        return self.output_layer(z)\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, in_channels=1, latent_dim=32, out_channels=1, num_classes=3, label_embedding_dim=16):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = VAEEncoder(in_channels, latent_dim, num_classes, label_embedding_dim)\n",
    "        self.decoder = VAEDecoder(latent_dim, out_channels, num_classes, label_embedding_dim)\n",
    "\n",
    "    def forward(self, x, label_embedding):\n",
    "        mu, log_var = self.encoder(x, label_embedding)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        recon_x = self.decoder(z, label_embedding)\n",
    "        return recon_x, mu, log_var\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "\n",
    "class VAEGenerator(nn.Module):\n",
    "    def __init__(self, in_channel=1, out_channel=1, latent_dim=32, num_classes=3, label_embedding_dim=16):\n",
    "        super(VAEGenerator, self).__init__()\n",
    "        self.vae = VAE(in_channel, latent_dim, out_channel, num_classes, label_embedding_dim)\n",
    "\n",
    "    def forward(self, x, label_embedding):\n",
    "        recon_x, mu, log_var = self.vae(x, label_embedding)\n",
    "        return recon_x, mu, log_var\n",
    "        \n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097a6de2-4b8c-4925-9acf-d51665a538ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class InceptionV3FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(InceptionV3FeatureExtractor, self).__init__()\n",
    "        \n",
    "        self.inception = inception_v3(pretrained=True, aux_logits=True)\n",
    "        self.inception.aux_logits = False  \n",
    "        self.inception.fc = nn.Identity()  \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.inception(x)  \n",
    "\n",
    "\n",
    "inception_model = InceptionV3FeatureExtractor().to(device)\n",
    "inception_model.eval()\n",
    "\n",
    "\n",
    "def get_inception_features(images, batch_size=16):\n",
    "\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(images), batch_size):\n",
    "            batch = images[i:i+batch_size].to(device)\n",
    "            \n",
    "            \n",
    "            if batch.size(1) == 1:\n",
    "                batch = batch.repeat(1, 3, 1, 1)  \n",
    "            \n",
    "            \n",
    "            batch = F.interpolate(batch, size=(299, 299), mode='bilinear')\n",
    "            \n",
    "            \n",
    "            pred = inception_model(batch)\n",
    "            \n",
    "            features.append(pred.cpu().numpy())\n",
    "    \n",
    "    return np.concatenate(features, axis=0)  \n",
    "\n",
    "\n",
    "def calculate_fid(real_features, generated_features):\n",
    "    \"\"\"compute FID\"\"\"\n",
    "    mu_real, sigma_real = np.mean(real_features, axis=0), np.cov(real_features, rowvar=False)\n",
    "    mu_gen, sigma_gen = np.mean(generated_features, axis=0), np.cov(generated_features, rowvar=False)\n",
    "    \n",
    "    \n",
    "    diff = mu_real - mu_gen\n",
    "    covmean, _ = sqrtm(sigma_real @ sigma_gen, disp=False)\n",
    "    \n",
    "    \n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    \n",
    "    fid = np.sum(diff ** 2) + np.trace(sigma_real + sigma_gen - 2 * covmean)\n",
    "    return fid\n",
    "\n",
    "\n",
    "file_path = '../data/2_subpatches.npy'\n",
    "data = np.load(file_path, mmap_mode='r')  \n",
    "\n",
    "\n",
    "real_slices = data[364:365]  \n",
    "\n",
    "\n",
    "real_slices = torch.from_numpy(real_slices).float().to(device)  \n",
    "\n",
    "\n",
    "vae_generator = VAEGenerator(in_channel=1, out_channel=1, latent_dim=32, num_classes=3, label_embedding_dim=16).to(device)\n",
    "vae_generator.load_state_dict(torch.load('vae_generator_epoch_100.pth', map_location=device))\n",
    "vae_generator.eval()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    real_data_tensor = real_slices  \n",
    "    label_embedding_layer = vae_generator.vae.encoder.label_emb\n",
    "    label_embedding = label_embedding_layer(torch.tensor([0], device=device))  \n",
    "    mu, log_var = vae_generator.vae.encoder(real_data_tensor, label_embedding)\n",
    "    z = vae_generator.vae.reparameterize(mu, log_var)\n",
    "    generated_images = vae_generator.vae.decoder(z, label_embedding)  \n",
    "\n",
    "\n",
    "real_features_list = []\n",
    "generated_features_list = []\n",
    "\n",
    "\n",
    "for i in range(real_slices.shape[2]):  \n",
    "    \n",
    "    real_slice = real_slices[:, :, i, :, :]  \n",
    "    \n",
    "    generated_slice = generated_images[0, 0, i, :, :].cpu().numpy()  \n",
    "    \n",
    "    smoothed_generated = gaussian_filter(generated_slice, sigma=1.5)\n",
    "    otsu_threshold = threshold_otsu(smoothed_generated)\n",
    "    binary_generated = (smoothed_generated > otsu_threshold).astype(np.float32)  \n",
    "    \n",
    "    real_slice_tensor = real_slice.clone().detach()  \n",
    "    \n",
    "    generated_slice_tensor = torch.from_numpy(binary_generated).float().unsqueeze(0).unsqueeze(0).to(device)  \n",
    "    \n",
    "    real_slice_tensor = real_slice_tensor.repeat(1, 3, 1, 1)  \n",
    "    generated_slice_tensor = generated_slice_tensor.repeat(1, 3, 1, 1)  \n",
    "    \n",
    "    real_features = get_inception_features(real_slice_tensor)  \n",
    "    generated_features = get_inception_features(generated_slice_tensor)  \n",
    "    \n",
    "    \n",
    "    real_features_list.append(real_features)\n",
    "    generated_features_list.append(generated_features)\n",
    "\n",
    "\n",
    "real_features = np.concatenate(real_features_list, axis=0)  \n",
    "generated_features = np.concatenate(generated_features_list, axis=0)  \n",
    "\n",
    "\n",
    "fid_value = calculate_fid(real_features, generated_features)\n",
    "print(f\"FID: {fid_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d084fd-ac3a-40ac-8095-19fda135c9c5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class InceptionV3FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(InceptionV3FeatureExtractor, self).__init__()\n",
    "        \n",
    "        self.inception = inception_v3(pretrained=True, aux_logits=True)\n",
    "        \n",
    "        self.inception.aux_logits = False\n",
    "        \n",
    "        self.inception.fc = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        output = self.inception(x)\n",
    "        if isinstance(output, tuple):\n",
    "            \n",
    "            main_output, _ = output\n",
    "            return main_output\n",
    "        else:\n",
    "            \n",
    "            return output\n",
    "\n",
    "\n",
    "inception_model = InceptionV3FeatureExtractor().to(device)\n",
    "inception_model.eval()\n",
    "\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "def min_max_normalize(tensor):\n",
    "    min_val = tensor.min()\n",
    "    max_val = tensor.max()\n",
    "    \n",
    "    normalized_tensor = (tensor - min_val) / (max_val - min_val + 1e-8)\n",
    "    return normalized_tensor\n",
    "\n",
    "\n",
    "def get_inception_features(images, batch_size=16):\n",
    "\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(images), batch_size):\n",
    "            batch = images[i:i+batch_size].to(device)\n",
    "\n",
    "            \n",
    "            if batch.size(1) == 1:\n",
    "                batch = batch.repeat(1, 3, 1, 1)  \n",
    "\n",
    "            \n",
    "            print(f\"Batch {i // batch_size + 1}:\")\n",
    "            print(f\"  Before normalize - min: {batch.min().item()}, max: {batch.max().item()}\")\n",
    "            batch = min_max_normalize(batch)\n",
    "            print(f\"  After normalize - min: {batch.min().item()}, max: {batch.max().item()}\")\n",
    "\n",
    "            \n",
    "            batch = F.interpolate(batch, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "\n",
    "            \n",
    "            batch = normalize(batch)\n",
    "\n",
    "            \n",
    "            pred = inception_model(batch)\n",
    "\n",
    "            features.append(pred.cpu().numpy())\n",
    "\n",
    "    return np.concatenate(features, axis=0)  \n",
    "\n",
    "\n",
    "def calculate_fid(real_features, generated_features):\n",
    "\n",
    "    mu_real, sigma_real = np.mean(real_features, axis=0), np.cov(real_features, rowvar=False)\n",
    "    mu_gen, sigma_gen = np.mean(generated_features, axis=0), np.cov(generated_features, rowvar=False)\n",
    "    \n",
    "    \n",
    "    diff = mu_real - mu_gen\n",
    "    covmean, _ = sqrtm(sigma_real @ sigma_gen, disp=False)\n",
    "    \n",
    "    \n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    \n",
    "    fid = np.sum(diff ** 2) + np.trace(sigma_real + sigma_gen - 2 * covmean)\n",
    "    return fid\n",
    "\n",
    "\n",
    "file_path = '../data/2_subpatches.npy'\n",
    "data = np.load(file_path, mmap_mode='r')  \n",
    "\n",
    "if data.min() < 0.0 or data.max() > 1.0:\n",
    "\n",
    "    data = data / 255.0  \n",
    "\n",
    "\n",
    "real_slices = data[364:365]  \n",
    "\n",
    "\n",
    "real_slices = torch.from_numpy(real_slices).float().to(device)  \n",
    "\n",
    "\n",
    "vae_generator = VAEGenerator(in_channel=1, out_channel=1, latent_dim=32, num_classes=3, label_embedding_dim=16).to(device)\n",
    "vae_generator.load_state_dict(torch.load('vae_generator_epoch_100.pth', map_location=device))\n",
    "vae_generator.eval()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    real_data_tensor = real_slices  \n",
    "    label_embedding_layer = vae_generator.vae.encoder.label_emb\n",
    "    label_embedding = label_embedding_layer(torch.tensor([0], device=device))  \n",
    "    mu, log_var = vae_generator.vae.encoder(real_data_tensor, label_embedding)\n",
    "    z = vae_generator.vae.reparameterize(mu, log_var)\n",
    "    generated_images = vae_generator.vae.decoder(z, label_embedding)  \n",
    "\n",
    "\n",
    "if generated_images.min() < 0.0 or generated_images.max() > 1.0:\n",
    "    print(\"The generated image is not within the range of [0, 1]. Perform normalization processing.\")\n",
    "    generated_images = min_max_normalize(generated_images)\n",
    "\n",
    "\n",
    "real_features_list = []\n",
    "generated_features_list = []\n",
    "\n",
    "\n",
    "for i in range(real_slices.shape[2]):  \n",
    "    \n",
    "    real_slice = real_slices[:, :, i, :, :]  \n",
    "\n",
    "    \n",
    "    generated_slice = generated_images[0, 0, i, :, :].cpu().numpy()  \n",
    "\n",
    "    \n",
    "    print(f\"min={generated_slice.min()}, max={generated_slice.max()}\")\n",
    "\n",
    "    \n",
    "    smoothed_generated = gaussian_filter(generated_slice, sigma=1.5)\n",
    "    otsu_threshold = threshold_otsu(smoothed_generated)\n",
    "    binary_generated = (smoothed_generated > otsu_threshold).astype(np.float32)  \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    real_slice_tensor = real_slice.clone().detach()  \n",
    "\n",
    "    \n",
    "    generated_slice_tensor = torch.from_numpy(binary_generated).float().unsqueeze(0).unsqueeze(0).to(device)  \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    real_slice_tensor = real_slice_tensor.repeat(1, 3, 1, 1)  \n",
    "    generated_slice_tensor = generated_slice_tensor.repeat(1, 3, 1, 1)  \n",
    "\n",
    "    \n",
    "    real_slice_tensor = min_max_normalize(real_slice_tensor)\n",
    "    generated_slice_tensor = min_max_normalize(generated_slice_tensor)\n",
    "\n",
    "    \n",
    "    real_features = get_inception_features(real_slice_tensor)  \n",
    "    generated_features = get_inception_features(generated_slice_tensor)  \n",
    "\n",
    "    \n",
    "    real_features_list.append(real_features)\n",
    "    generated_features_list.append(generated_features)\n",
    "\n",
    "\n",
    "real_features = np.concatenate(real_features_list, axis=0)  \n",
    "generated_features = np.concatenate(generated_features_list, axis=0)  \n",
    "\n",
    "\n",
    "fid_value = calculate_fid(real_features, generated_features)\n",
    "print(f\"FID: {fid_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814c5f35-d843-4f74-b6c5-854791037783",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_path = '../data/2_subpatches.npy'\n",
    "data = np.load(file_path, mmap_mode='r')\n",
    "real_slices = data[364:365]\n",
    "print(real_slices.shape)\n",
    "\n",
    "subpatch_count = data.shape[0]  \n",
    "print(f\"subpatch_num: {subpatch_count}\")\n",
    "\n",
    "subpatch_shape = data.shape[1:]  \n",
    "print(f\"subpatch_size: {subpatch_shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rockgan",
   "language": "python",
   "name": "rockgan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
